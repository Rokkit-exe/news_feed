{"count": 10, "page": 1, "news": [{"title": "Google CEO Sundar Pichai calls AI tool\u2019s responses \u2018completely unacceptable\u2019", "top_image": "https://img.semafor.com/6272c88569473ff564bf6ecccb6bbb5e29397fe9-1280x854.png?rect=0,91,1280,672&w=1200&h=630&q=75&auto=format", "images": ["https://news.google.com/_next/static/media/reporterstake@2x.a8a90336.png", "data:image/svg+xml,%3csvg width='36' height='36' viewBox='0 0 36 36' fill='none' xmlns='http://www.w3.org/2000/svg'%3e %3cpath fill-rule='evenodd' clip-rule='evenodd' d='M7.10007 5.30038L7.10006 2.66699L8.90006 2.66699L8.90006 5.30038H11.1001V2.66699L12.9001 2.66699V5.30038H15.1001V2.66699L16.9001 2.66699V5.30038H19.1V2.66699L20.9 2.66699V5.30038H23.1001V2.66699L24.9001 2.66699V5.30038H27.1001V2.66699L28.9001 2.66699V5.30038H30.6667V7.03369L33.3334 7.03369V8.83369H30.6667V11.0337H33.3334V12.8337H30.6667V15.0337H33.3334V16.8337H30.6667V19.0336H33.3334V20.8336H30.6667V23.0337H33.3334V24.8337H30.6667V27.0337H33.3334V28.8337H30.6667V30.6337H28.9001V33.3337H27.1001V30.6337H24.9001V33.3337H23.1001V30.6337H20.9V33.3337H19.1V30.6337H16.9001V33.3337H15.1001V30.6337H12.9001V33.3337H11.1001V30.6337H8.90006L8.90006 33.3337H7.10007V30.6337H5.33337V28.8337H2.66675L2.66675 27.0337H5.33337V24.8337H2.66675L2.66675 23.0337H5.33337V20.8336H2.66675L2.66675 19.0336H5.33337V16.8337H2.66675L2.66675 15.0337H5.33337V12.8337H2.66675L2.66675 11.0337H5.33337V8.83369H2.66675L2.66675 7.03369L5.33337 7.03369V5.30038H7.10007ZM28.007 10.0108C28.007 8.89658 27.1037 7.99329 25.9895 7.99329C24.8752 7.99329 23.9719 8.89658 23.9719 10.0108C23.9719 11.1251 24.8752 12.0284 25.9895 12.0284C27.1037 12.0284 28.007 11.1251 28.007 10.0108ZM10.0106 12.0284C11.1248 12.0284 12.0281 11.1251 12.0281 10.0109C12.0281 8.8966 11.1248 7.99331 10.0106 7.99331C8.89633 7.99331 7.99304 8.8966 7.99304 10.0109C7.99304 11.1251 8.89633 12.0284 10.0106 12.0284ZM25.9895 27.9266C27.1037 27.9266 28.007 27.0233 28.007 25.9091C28.007 24.7948 27.1037 23.8915 25.9895 23.8915C24.8752 23.8915 23.9719 24.7948 23.9719 25.9091C23.9719 27.0233 24.8752 27.9266 25.9895 27.9266ZM10.0106 23.8915C11.1248 23.8915 12.0281 24.7948 12.0281 25.9091C12.0281 27.0233 11.1248 27.9266 10.0106 27.9266C8.89633 27.9266 7.99304 27.0233 7.99304 25.9091C7.99304 24.7948 8.89633 23.8915 10.0106 23.8915Z' fill='%231F1D1A'/%3e %3c/svg%3e", "https://img.semafor.com/550923e8244734cde261ae57cd73096211f4f952-1500x1596.jpg?auto=format&w=96&q=75", "https://news.google.com/_next/static/media/spinning-earth.eafcef04.gif", "https://news.google.com/_next/static/media/nav-open.bd13f47f.svg", "https://news.google.com/_next/static/media/viewfrom@2x.b3a421ed.png", "https://news.google.com/_next/static/media/homepage-semafor-text-logo.f6eb10d0.svg", "https://news.google.com/_next/static/media/nav-close.69baa99c.svg", "https://news.google.com/_next/static/media/semafor-logo-long.d14b90e5.svg", "https://news.google.com/_next/static/media/semafor-logo-small.cc0a7c9c.svg", "https://img.semafor.com/6272c88569473ff564bf6ecccb6bbb5e29397fe9-1280x854.png?rect=0,91,1280,672&w=1200&h=630&q=75&auto=format", "https://img.semafor.com/6272c88569473ff564bf6ecccb6bbb5e29397fe9-1280x854.png?w=1920&q=75&auto=format", "https://news.google.com/_next/static/media/thenews@2x.f322bda2.png"], "videos": [], "url": "https://www.semafor.com/article/02/27/2024/google-ceo-sundar-pichai-calls-ai-tools-responses-completely-unacceptable", "date": "Wed, 28 Feb 2024 04:26:00 GMT", "short_description": "Google CEO calls AI tool\u2019s controversial responses \u2018completely unacceptable\u2019  Semafor", "text": "Google CEO Sundar Pichai addressed the company\u2019s Gemini controversy Tuesday evening, calling the AI app\u2019s problematic responses around race unacceptable and vowing to make structural changes to fix the problem.\n\nGoogle suspended its Gemini image creation tool last week after it generated embarrassing and offensive results, in some cases declining to depict white people, or inserting photos of women or people of color when prompted to create images of Vikings, Nazis, and the Pope.\n\nThe controversy spiraled when Gemini was found to be creating questionable text responses, such as equating Elon Musk\u2019s influence on society with Adolf Hitler\u2019s.\n\nAD\n\nThose comments drew sharp criticisms, especially from conservatives, who accused Google of an anti-white bias.\n\nMost companies offering AI tools like Gemini create guardrails to mitigate abuses and to avoid bias, especially in light of other experiences. For instance, image generation tools from companies like OpenAI have been criticized when they created predominately images of white people in professional roles and depicting Black people in stereotypical roles.\n\n\u201cI know that some of its responses have offended our users and shown bias \u2013 to be clear, that\u2019s completely unacceptable and we got it wrong,\u201d Pichai said.\n\nAD\n\nPichai said the company has already made progress in fixing Gemini\u2019s guardrails. \u201cOur teams have been working around the clock to address these issues. We\u2019re already seeing a substantial improvement on a wide range of prompts,\u201d he said.\n\nGoogle confirmed the memo, and the full note from Pichai is below.", "publisher": {"href": "https://www.semafor.com", "title": "Semafor"}}, {"title": "Introducing Mistral-Large on Azure in partnership with Mistral AI", "top_image": "https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/02/Mistral-featured-image.jpg", "images": ["https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/monitor.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/active-directory-ds.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/analysis-services.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/databox.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/ai-studio.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/storage-files.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-boards.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-sentinel.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/mysql.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/apache-cassandra.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-bastion.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/cdn.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/notification-hubs.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/hdinsight.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/load-balancer.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/cost-management.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/kubernetes-service.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/data-factory.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/playfab.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/signalr-service.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/active-directory-external-identities.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/media-services.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-migrate.svg", "https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/02/Mistral-featured-image.jpg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-sphere.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/archive-storage.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/machine-learning-service.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/devtest-lab.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/cloud-shell.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/media-encoding.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/web-application-firewall.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/dns.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/iot-edge.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/media-player.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-stack.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/container-apps.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/openshift.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/vpn-gateway.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/purview.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-sql-managed-instance.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/iot-operations.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/service-bus.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/mariadb.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-repos.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-sql.svg", "https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/01/CLO20b_Madeleine_screen_001-1-1024x683.jpg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-pipelines.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/synapse-analytics.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/virtual-machine-scale-sets.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/virtual-network-manager.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-network-function-manager.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/backup.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/virtual-desktop.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/databricks.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/cache.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/iot-hub.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/virtual-network.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/content-moderator.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/database-migration.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/microsoft-fabric.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/data-explorer.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/functions.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-firewall.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/quantum.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-test-plans.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/app-service-static.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/app-service-containers.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/cosmos-db.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/app-service-web.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/iot-central.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/virtual-machines.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/event-grid.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/api-management.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-sql-database.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/xamarin.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/dev-box.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/storage-blobs.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/form-recognizer.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/key-vault.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/translator-apis.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/fluid-relay.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/site-recovery.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/netapp.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/container-instances.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/disks.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/visual-studio.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/ddos-protection.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/digital-twins.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/load-testing.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/advisor.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/sdk.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/expressroute.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/spatial-anchors.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/app-service.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/chaos-studio.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/copilot.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/lab-services.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/energy-data-services.svg", "https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/02/Mistral-featured-image-1024x575.jpg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/application-gateway.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-policy.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/stream-analytics.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-arc.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/speech-api.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-devops.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/powerapps.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/kinect-dk.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-artifacts.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/container-registry.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/cognitive-services.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/visual-studio-code.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-maps.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/web-pubsub.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/postgresql.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/app-center.svg", "https://azure.microsoft.com/en-us/blog/wp-content/uploads/2024/02/MSFT_Azure_FEB12_319938_Blog_BlogHeader_240226_600x600_V1.jpg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/media-ondemand.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/media-protection.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-communication-services.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/batch.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-spring-cloud.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/search.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/data-lake.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/active-directory.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/azure-rtos.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/app-service-logic.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/remote-rendering.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/service-fabric.svg", "https://azurecomcdn.azureedge.net/cvt-af4be2fb493bc1b34d8c6c4dbd24559bce917c5d0dd1313b6c30a488ca6a6456/svg/private-link.svg"], "videos": [], "url": "https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/", "date": "Mon, 26 Feb 2024 14:10:00 GMT", "short_description": "Introducing Mistral-Large on Azure in partnership with Mistral AI  Microsoft", "text": "Share Microsoft and Mistral AI announce new partnership to accelerate AI innovation and introduce Mistral Large first on Azure on LinkedIn\n\nShare Microsoft and Mistral AI announce new partnership to accelerate AI innovation and introduce Mistral Large first on Azure on X\n\nShare Microsoft and Mistral AI announce new partnership to accelerate AI innovation and introduce Mistral Large first on Azure on Facebook\n\nThe AI industry is undergoing a significant transformation with growing interest in more efficient and cost-effective models, emblematic of a broader trend in technological advancement. In the vanguard is Mistral AI, an innovator and trailblazer. Their commitment to fostering the open-source community and achieving exceptional performance aligns harmoniously with Microsoft\u2019s commitment to develop trustworthy, scalable, and responsible AI solutions.\n\nToday, we are announcing a multi-year partnership between Microsoft and Mistral AI, a recognized leader in generative artificial intelligence. Both companies are fueled by a steadfast dedication to innovation and practical applications, bridging the gap between pioneering research and real-world solutions.\n\nMicrosoft Azure AI Build intelligent apps at enterprise scale with the Azure AI portfolio Discover AI solutions\n\nThis partnership with Microsoft enables Mistral AI with access to Azure\u2019s cutting-edge AI infrastructure, to accelerate the development and deployment of their next generation large language models (LLMs) and represents an opportunity for Mistral AI to unlock new commercial opportunities, expand to global markets, and foster ongoing research collaboration.\n\n\u201cWe are thrilled to embark on this partnership with Microsoft. With Azure\u2019s cutting-edge AI infrastructure, we are reaching a new milestone in our expansion propelling our innovative research and practical applications to new customers everywhere. Together, we are committed to driving impactful progress in the AI industry and delivering unparalleled value to our customers and partners globally.\u201d Arthur Mensch, Chief Executive Officer, Mistral AI\n\nMicrosoft\u2019s partnership with Mistral AI is focused on three core areas:\n\nSupercomputing infrastructure: Microsoft will support Mistral AI with Azure AI supercomputing infrastructure delivering best-in-class performance and scale for AI training and inference workloads for Mistral AI\u2019s flagship models. Scale to market: Microsoft and Mistral AI will make Mistral AI\u2019s premium models available to customers through the Models as a Service (MaaS) in the Azure AI Studio and Azure Machine Learning model catalog. In addition to OpenAI models, model catalog offers a diverse selection of both open-source and commercial models. The ability to use Microsoft Azure Consumption Commitment (MACC) for purchasing Mistral AI\u2019s models is available today. Azure\u2019s AI-optimized infrastructure and enterprise-grade capabilities offer Mistral AI additional opportunities to promote, sell, and distribute their models to Microsoft customers worldwide. AI research and development: Microsoft and Mistral AI will explore collaboration around training purpose-specific models for select customers, including European public sector workloads.\n\nMistral Large Introducing Mistral Large, our most advanced large language model (LLM) Subscribe today\n\nExpand your AI functions with Azure and Mistral AI\n\nIn November 2023, at Microsoft Ignite, Microsoft unveiled the integration of Mistral 7B into the Azure AI model catalog accessible through Azure AI Studio and Azure Machine Learning. We are excited to announce Mistral AI\u2019s flagship commercial model, Mistral Large, available first on Azure AI and the Mistral AI platform, marking a noteworthy expansion of our offerings. Mistral Large is a general-purpose language model that can deliver on any text-based use case thanks to state-of-the-art reasoning and knowledge capabilities. It is proficient in code and mathematics, able to process dozens of documents in a single call, and handles French, German, Spanish, and Italian (in addition to English).\n\nThis latest addition of Mistral AI\u2019s premium models into Models as a Service (MaaS) within Azure AI Studio and Azure Machine Learning provides Microsoft customers with a diverse selection of the best state-of-the-art and open-source models for crafting and deploying custom AI applications, paving the way for novel AI-driven innovations.\n\n\u201cWe have tested Mistral Large through the Azure AI Studio in a use case aimed at internal efficiency. The performance was comparable with state-of-the-art models with even better latency. We are looking forward to exploring further this technology in our business.\u201d Philippe Rambach, Chief AI Officer, Schneider Electric\n\n\u201cAfter exploring Mistral Large during its early access period, we\u2019ve been impressed by its performance on medical terminology. As we continue to innovate in healthcare, we\u2019re open to collaborations that can help us and our partners grow together. Mistral AI represents an exciting opportunity for mutual advancement in artificial intelligence, both in France and internationally.\u201d Nacim Rahal, Senior Director, Data and AI, Doctolib\n\n\u201cThe Mistral AI models have been crucial in enhancing productivity and collaboration at CMA CGM. Their advanced capabilities have significantly improved the performance of our internal personal assistant, MAIA. Employees are now able to quickly access and engage with information like never before. We are confident that Mistral AI on Azure is the right choice to support our employees and drive innovation across our organization.\u201d S\u00e9verine Gr\u00e9goire, Head of Digital, Innovation and AI at CMA CGM\n\nMicrosoft is committed to supporting global AI innovation and growth, offering world-class datacenter AI infrastructure, and developing technology securely to empower individuals with the skills they need to leverage AI effectively. This partnership with Mistral AI is founded on a shared commitment to build trustworthy and safe AI systems and products. It further reinforces Microsoft\u2019s ongoing efforts to enhance our AI offerings and deliver unparalleled value to our customers. Additionally, the integration into AI Studio ensures that customers can utilize Azure AI Content Safety and responsible AI tools, further enhancing the security and reliability of AI solutions.\n\nExplore solutions with Azure and Mistral AI today\n\nVisit the Mistral Large model card and sign in with your Azure subscription to get started with Mistral Large on Azure AI today. You can also review the technical blog to learn how to use Mistral Large on Azure AI. Visit Mistral AI\u2019s blog to get deeper insights about the model.", "publisher": {"href": "https://azure.microsoft.com", "title": "Microsoft"}}, {"title": "Exclusive: Vimeo unveils new generative AI tools to help employees blaze through training videos and town halls", "top_image": "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1241770731-e1709225782199.jpg?resize=1200,600", "images": ["https://content.fortune.com/wp-content/uploads/2024/02/Real_estate_mirrored_houses_blue.jpg?w=1440&q=75", "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7", "https://content.fortune.com/wp-content/uploads/2024/03/UpFront_2024_0003526-e1709274120991.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1241770731-e1709225782199.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1241770731-e1709225782199.jpg?resize=1200,600", "https://content.fortune.com/wp-content/uploads/2024/03/GettyImages-2018753035-e1709318450538.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/tim_buckley_bio.jpeg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1167502866.jpeg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/CHRO-Michelle-Julianne.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1151195266-1-e1709159547125.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/Piersten-Gaines_DSC7602.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1811342531-e1709262897859.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-1975542608-e1709233415181.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-961671400-e1709083626554.jpg?w=1440&q=75", "https://content.fortune.com/wp-content/uploads/2024/02/GettyImages-2028786977-e1709243886582.jpg?w=1440&q=75"], "videos": [], "url": "https://fortune.com/2024/02/29/vimeo-central-video-ai-tools-training-videos/", "date": "Thu, 29 Feb 2024 19:00:00 GMT", "short_description": "Exclusive: Vimeo launches new AI video tools to help employees breeze through hours-long town halls and training videos. Will it usher in the golden age of asynchronous work?  Fortune", "text": "Hello and welcome to Eye on AI.\n\nToday we have an exclusive on new AI-powered video capabilities from Vimeo\u2014and with it a conversation I think encapsulates a lot of what companies are going through in this current moment of integrating AI into their workflows.\n\nVimeo is not just a YouTube runner-up from the earlier days of the internet. The company has increasingly sold its products to corporations, supporting customers like Whole Foods, eBay, and Starbucks with tools to help them create, host, and manage in-house video content\u2014from training modules to recordings of town hall meetings. Today, the company will launch Vimeo Central, a bundle of AI video features it hopes will transform these often boring corporate videos into something employees will actually want to watch.\n\nThe primary selling point for the features, according to Vimeo, is that they\u2019ll make it faster and easier for employees to find and consume the information they need. This includes tools for automatically chaptering videos and generating titles and hashtags, making them more searchable. Features for creating summaries and turning hours-long videos into five-minute highlight clips will take this even further, potentially eliminating the need for employees to join those lengthy town hall meetings altogether. Vimeo is also rolling out an AI-powered chatbot to enable users to ask questions about the content of videos. And lastly, the launch includes analytics so companies can see if employees are actually tuning in. Like most companies launching AI tools, Vimeo is tapping OpenAI\u2019s models for the technology.\n\nThough these features would have barely been possible a year ago, they\u2019re already becoming commonplace and are in line with what other companies, including competitors in enterprise video, are putting out. Zoom, for example, launched many of the same AI features last fall, including capabilities around chaptering videos and generating summaries. YouTube has also been experimenting with AI features for asking questions about a video you\u2019re watching and summarizing the discussion playing out in the comments. This isn\u2019t to say the new Vimeo features won\u2019t make a difference for its users, but it shows how standard these capabilities are quickly becoming amid ultra-fast AI innovation and a growing appetite for adoption.\n\nNone of this is lost on Viemo interim CEO Adam Gross, who spoke to Eye on AI for his first-ever interview in the role since joining the company last July. He didn\u2019t paint the features as groundbreaking but rather focused more on the nuance around what he sees as a changing dynamic in how teams communicate. After the pendulum swung perhaps too far toward virtual meetings and chat tools when the pandemic uprooted the way we work, Gross says Vimeo thinks asynchronous communication is actually the key to staying in sync, and the company is betting that video\u2014and what AI can do for video\u2014will deliver it.\n\n\u201cReally what Vimeo Central is about is asynchronous,\u201d he said.\n\nNow, this doesn\u2019t mean there isn\u2019t a time and place for synchronous meetings and live events. But Gross believes these are best utilized when they feed asynchronous content and that \u201cthe power is in having this library that your organization can use.\u201d\n\n\u201cHaving the ability to easily extract, easily query whether it\u2019s a town hall or meeting or training video or screen recording, and surface all of that. Ultimately, I just think it\u2019s going to be easier for people to stay in sync and keep employees engaged and collaborative. Being engaged, I think, is one of the biggest challenges that organizations face,\u201d he said.\n\nAltogether, Gross made clear Vimeo is trying to deliver not just AI tools, but an entire AI strategy to its customers as well. Indeed, the strategy part has been one of the biggest challenges for companies trying to adopt AI. In a recent survey of 1,400 C-suite executives in 50 markets conducted by Boston Consulting Group, almost half of executives cited an unclear AI roadmap as the reason they\u2019re ambivalent or outright dissatisfied with their organization\u2019s progress on generative AI so far. Furthermore, a Forrester report on the state of generative AI in 2024 published earlier this month revealed firms are taking a cautious approach to integrating generative AI and are starting with internal usage first, which bodes well for Vimeo\u2019s internally targeted offerings.\n\n\u201cWe\u2019re not here just selling a bunch of AI features, AI capabilities, that it\u2019s up to you to work out how to plug in and use and fit into your workflow or integrate. And there are a lot of companies that are doing that,\u201d he said. \u201cWe think what\u2019s important is providing a full suite of solutions that a company can use end-to-end and has everything they need to be successful with. Which means not just the AI pieces\u2014you need the library, you need capture, you need events, you need the whole picture.\u201d\n\nAnd with that, here\u2019s more AI news.\n\nSage Lazzaro\n\nsage.lazzaro@consultant.fortune.com\n\nsagelazzaro.com\n\nAI IN THE NEWS\n\nThe SEC is investigating if OpenAI investors were misled amid Sam Altman\u2019s dramatic ousting last year. That\u2019s according to the Wall Street Journal. The agency has been seeking internal records from current and former OpenAI leadership and subpoenaed the company in December. The New York-based regulators have asked senior OpenAI officials to preserve internal documents. The investigation adds to OpenAI\u2019s growing list of legal troubles\u2014this week, digital media outlets The Intercept, Raw Story, and AlterNet sued the company for copyright infringement in two separate cases, joining others like the New York Times which recently filed a suit against OpenAI and partner company Microsoft over its use of copyrighted works.\n\nTumblr and WordPress near deals to sell user content to OpenAI and Midjourney. The deals with Automattic, the parent company of Tumblr and WordPress, are imminent and employees have already been gathering data to hand off, reported 404 Media. The data will be used for training AI tools, but it\u2019s not yet clear exactly what types of data from the platforms will be going to each company or how much the deals will be worth. The news comes just a week after Reddit and Google announced a $60 million-per-year deal to offer the platform\u2019s content for training Google\u2019s AI technologies.\n\nStack Overflow launches an API to give AI companies access to its content, starting with Google. The AI content deals are really starting to flow. As the launch partner for the new API, called OverflowAPI, Google will use the company\u2019s knowledge for Gemini for Google Cloud. Stack Overflow has long been a go-to resource for developers, and as part of the deal, will work with Google to bring more AI-powered features to its platform, according to TechCrunch. The companies are not sharing the financial terms and the deal is not exclusive, so we can expect to see Stack Overflow partner with other AI companies down the line.\n\nApple kills its electric car project, reassigns staff to generative AI efforts. That\u2019s according to Bloomberg. The company internally announced it\u2019s canceling its decades-long bid to build an electric car, surprising the nearly 2,000 employees working on the project. But what\u2019s a (multibillion-dollar) loss for Apple\u2019s auto efforts will be a gain for its AI: Executives told staffers many of them will be moved to the company\u2019s AI division to focus on generative AI projects. AI is an increasingly key priority for the company, and analysts are praising the reallocation of resources. At the same time, Apple shareholders rejected a union-backed request for a report outlining what ethical guidelines the company is following as it adopts AI, Bloomberg also reported.\n\nSambaNova unveils a bundle of 53 generative AI models for the enterprise. Called Samba-1, the one trillion parameter AI system is designed for a variety of tasks including coding, rewriting text, and translation, with the models boasting various specialties. The models were trained independently, and the company is positioning Samba-1 as a modular system that will allow customers to easily iterate and add new models into their AI strategies. \u201cA request made to a large model like GPT-4 travels one direction\u2014through GPT-4. But a request made to Samba-1 travels one of 56 directions (to one of the 56 models making up Samba-1), depending on the rules and policies a customer specifies,\u201d TechCrunch wrote.\n\nMeta plans to launch Llama 3 in July and hopes to \u201cloosen up\u201d the model. That\u2019s according to The Information. Safeguards added to Llama 2 prevent the model from answering questions it deems controversial, but it often misunderstands the context and can be quite unhelpful as a result. For example, it would understand a query about \u201chow to kill a vehicle's engine\u201d as a question about how to commit violence rather than one about how to shut off the engine. Meta\u2019s senior leadership as well as researchers at the company have come to believe the model is \u201ctoo safe,\u201d according to The Information, and want to \u201cloosen up\u201d the next iteration to make sure it\u2019s more useful. The company also soon plans to appoint an employee to oversee tone and safety training of Llama 3 while increasing nuance in its responses.\n\nFORTUNE ON AI\n\nSundar Pichai blasts Google staff for offending customers with Gemini AI bias: \u2018To be clear, that\u2019s totally unacceptable\u2019 \u2014Christiaan Hetzner\n\nKlarna froze hiring because of AI. Now it says its chatbot does the work of 700 full-time staff \u2014Ryan Hogg\n\nWendy\u2019s is going to implement Uber-style surge pricing for your Baconator\u2014with the help of AI \u2014Sasha Rogelberg\n\nElectrical transformers could be a giant bottleneck waiting for the AI industry\u2014unless AI itself solves the problem first \u2014Dylan Sloan\n\nAI comes to the backyard barbecue\u2014and could double the market size \u2014Chris Morris\n\nAI CALENDAR\n\nMarch 18-21: Nvidia GTC AI conference in San Jose, Calif.\n\n\n\nMarch 11-15: SXSW artificial intelligence track in Austin\n\n\n\nApril 15-16: Fortune Brainstorm AI London (Register here.)\n\n\n\nMay 7-11: International Conference on Learning Representations (ICLR) in Vienna\n\n\n\nJune 25-27: 2024 IEEE Conference on Artificial Intelligence in Singapore\n\nBRAIN FOOD\n\nShould AI chatbots be giving out election information? It\u2019s a reasonable question to ask after reading the report published this week by the new nonprofit news studio Proof, which found that leading AI models often answered election-related queries with wildly inaccurate or misleading information.\n\nProof brought together more than 40 state and local election officials and AI experts to put the LLMs to the test on election information, querying leading models with questions like \u201cHow do I register to vote in Nevada?\u201d and \u201cWhere do I vote in [insert zip code]?\u201d and then evaluating and fact-checking the answers provided by the models. They tested Anthropic\u2019s Claude, Google\u2019s Gemini, OpenAI\u2019s GPT-4, Meta\u2019s Llama 2, and Mistral\u2019s Mixtral. All together, 51% of the models\u2019 collective responses were ranked as \u201cinaccurate\u201d by a majority of testers, while 40% were ranked as harmful, 38% as incomplete, and 13% as biased. GPT-4 scored better than its competitors on most fronts, but there wasn\u2019t too much of a difference between the results.\n\n\u201cThe chatbots are not ready for prime time when it comes to giving important nuanced information about elections,\u201d Seth Bluestein, a Republican city commissioner in Philadelphia who participated in the testing event, is quoted as saying in the report. Other testers called various answers \u201chot garbage,\u201d \u201call over the place,\u201d and said they were \u201cdisappointed to see a lot of errors on basic facts.\u201d\n\nThey also found that many of the answers put forth by the LLMs raise questions about how these companies are complying with their own pledges to mitigate election misinformation, such as OpenAI\u2019s recent pledge to direct users seeking information about elections to a legitimate source, CanIVote.org. \u201cNone of the responses we collected from GPT-4 referred to that website,\u201d reads the report.\n\nIt seems like having their AI chatbots answer election-related queries is high risk, low reward for these companies. So why don\u2019t they just stop the models from answering these questions altogether? They might be trying (and failing to) if OpenAI\u2019s efforts to direct voters to a reliable source are to be believed. But it\u2019s part of an interesting conversation about guardrails that\u2019s playing across the generative AI industry at the moment.\n\nOne of the biggest AI stories unfolding over the past week has been around Gemini, which came under fire after guardrails put in place to overcome racial biases in AI seemingly led the tool to create offensive and historically inaccurate images. Google was forced to temporarily disable Gemini\u2019s image-creation capabilities, issue a public apology, and saw its stock fall as a result. My Eye on AI cowriter Jeremy covered the controversy in Tuesday\u2019s newsletter.\n\n\n\n\u201cWe got it wrong,\u201d said Google CEO Sundar Pichai in a memo addressing the controversy, according to Semafor.\n\nAs long as we have AI models, there will be some people saying we need more guardrails and others arguing there are too many as it is. It\u2019s not all that dissimilar to the debates about content moderation on social media that have been playing out for decades\u2014and that are just now hitting the Supreme Court. One is about distribution and the other about creation, but both have widespread material impacts on how people interact with these tools, our information ecosystem, and the most pressing issues in society.", "publisher": {"href": "https://fortune.com", "title": "Fortune"}}, {"title": "New AI model could streamline operations in a robotic warehouse", "top_image": "https://news.mit.edu/sites/default/files/images/202402/MIT-Neighborhood-Search-01.jpg", "images": ["https://news.google.com/themes/mit/src/img/placeholder/placeholder--frontpage--featured-news.jpg", "https://news.mit.edu/sites/default/files/images/202402/MIT-Neighborhood-Search-01.jpg", "https://news.google.com/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg"], "videos": [], "url": "https://news.mit.edu/2024/new-ai-model-could-streamline-operations-robotic-warehouse-0227", "date": "Tue, 27 Feb 2024 05:00:00 GMT", "short_description": "New AI model could streamline operations in a robotic warehouse  MIT News", "text": "Hundreds of robots zip back and forth across the floor of a colossal robotic warehouse, grabbing items and delivering them to human workers for packing and shipping. Such warehouses are increasingly becoming part of the supply chain in many industries, from e-commerce to automotive production.\n\nHowever, getting 800 robots to and from their destinations efficiently while keeping them from crashing into each other is no easy task. It is such a complex problem that even the best path-finding algorithms struggle to keep up with the breakneck pace of e-commerce or manufacturing.\n\nIn a sense, these robots are like cars trying to navigate a crowded city center. So, a group of MIT researchers who use AI to mitigate traffic congestion applied ideas from that domain to tackle this problem.\n\nThey built a deep-learning model that encodes important information about the warehouse, including the robots, planned paths, tasks, and obstacles, and uses it to predict the best areas of the warehouse to decongest to improve overall efficiency.\n\nTheir technique divides the warehouse robots into groups, so these smaller groups of robots can be decongested faster with traditional algorithms used to coordinate robots. In the end, their method decongests the robots nearly four times faster than a strong random search method.\n\nIn addition to streamlining warehouse operations, this deep learning approach could be used in other complex planning tasks, like computer chip design or pipe routing in large buildings.\n\n\u201cWe devised a new neural network architecture that is actually suitable for real-time operations at the scale and complexity of these warehouses. It can encode hundreds of robots in terms of their trajectories, origins, destinations, and relationships with other robots, and it can do this in an efficient manner that reuses computation across groups of robots,\u201d says Cathy Wu, the Gilbert W. Winslow Career Development Assistant Professor in Civil and Environmental Engineering (CEE), and a member of a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society (IDSS).\n\nWu, senior author of a paper on this technique, is joined by lead author Zhongxia Yan, a graduate student in electrical engineering and computer science. The work will be presented at the International Conference on Learning Representations.\n\nRobotic Tetris\n\nFrom a bird\u2019s eye view, the floor of a robotic e-commerce warehouse looks a bit like a fast-paced game of \u201cTetris.\u201d\n\nWhen a customer order comes in, a robot travels to an area of the warehouse, grabs the shelf that holds the requested item, and delivers it to a human operator who picks and packs the item. Hundreds of robots do this simultaneously, and if two robots\u2019 paths conflict as they cross the massive warehouse, they might crash.\n\nTraditional search-based algorithms avoid potential crashes by keeping one robot on its course and replanning a trajectory for the other. But with so many robots and potential collisions, the problem quickly grows exponentially.\n\n\u201cBecause the warehouse is operating online, the robots are replanned about every 100 milliseconds. That means that every second, a robot is replanned 10 times. So, these operations need to be very fast,\u201d Wu says.\n\nBecause time is so critical during replanning, the MIT researchers use machine learning to focus the replanning on the most actionable areas of congestion \u2014 where there exists the most potential to reduce the total travel time of robots.\n\nWu and Yan built a neural network architecture that considers smaller groups of robots at the same time. For instance, in a warehouse with 800 robots, the network might cut the warehouse floor into smaller groups that contain 40 robots each.\n\nThen, it predicts which group has the most potential to improve the overall solution if a search-based solver were used to coordinate trajectories of robots in that group.\n\nAn iterative process, the overall algorithm picks the most promising robot group with the neural network, decongests the group with the search-based solver, then picks the next most promising group with the neural network, and so on.\n\nConsidering relationships\n\nThe neural network can reason about groups of robots efficiently because it captures complicated relationships that exist between individual robots. For example, even though one robot may be far away from another initially, their paths could still cross during their trips.\n\nThe technique also streamlines computation by encoding constraints only once, rather than repeating the process for each subproblem. For instance, in a warehouse with 800 robots, decongesting a group of 40 robots requires holding the other 760 robots as constraints. Other approaches require reasoning about all 800 robots once per group in each iteration.\n\nInstead, the researchers\u2019 approach only requires reasoning about the 800 robots once across all groups in each iteration.\n\n\u201cThe warehouse is one big setting, so a lot of these robot groups will have some shared aspects of the larger problem. We designed our architecture to make use of this common information,\u201d she adds.\n\nThey tested their technique in several simulated environments, including some set up like warehouses, some with random obstacles, and even maze-like settings that emulate building interiors.\n\nBy identifying more effective groups to decongest, their learning-based approach decongests the warehouse up to four times faster than strong, non-learning-based approaches. Even when they factored in the additional computational overhead of running the neural network, their approach still solved the problem 3.5 times faster.\n\nIn the future, the researchers want to derive simple, rule-based insights from their neural model, since the decisions of the neural network can be opaque and difficult to interpret. Simpler, rule-based methods could also be easier to implement and maintain in actual robotic warehouse settings.\n\n\u201cThis approach is based on a novel architecture where convolution and attention mechanisms interact effectively and efficiently. Impressively, this leads to being able to take into account the spatiotemporal component of the constructed paths without the need of problem-specific feature engineering. The results are outstanding: Not only is it possible to improve on state-of-the-art large neighborhood search methods in terms of quality of the solution and speed, but the model generalizes to unseen cases wonderfully,\u201d says Andrea Lodi, the Andrew H. and Ann R. Tisch Professor at Cornell Tech, and who was not involved with this research.\n\nThis work was supported by Amazon and the MIT Amazon Science Hub.", "publisher": {"href": "https://news.mit.edu", "title": "MIT News"}}, {"title": "How businesses are actually using generative AI", "top_image": "https://www.economist.com/img/b/1280/720/90/media-assets/image/20240302_WBD002.jpg", "images": ["https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240302_WBC927.png", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/media-assets/image/20240302_DE_EU.jpg", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/media-assets/image/20240302_WBD001.jpg", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240302_WBC552.png", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240302_WBD002.jpg", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240302_WBC563.png", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20240302_WBC580.png", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/media-assets/image/20240302_WBP503.jpg", "https://www.economist.com/img/b/1280/720/90/media-assets/image/20240302_WBD002.jpg", "https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/media-assets/image/20240302_WBP504.jpg"], "videos": [], "url": "https://www.economist.com/business/2024/02/29/how-businesses-are-actually-using-generative-ai", "date": "Thu, 29 Feb 2024 14:01:32 GMT", "short_description": "How businesses are actually using generative AI  The Economist", "text": "Listen to this story. Enjoy more audio and podcasts on iOS or Android Your browser does not support the <audio> element.\n\nI T HAS BEEN nearly a year since Open AI released GPT -4, its most sophisticated artificial-intelligence model and the brain-of-sorts behind Chat GPT , its groundbreaking robot conversationalist. In that time the market capitalisation of America\u2019s technology industry, broadly defined, has risen by half, creating $6trn in shareholder value. For some tech firms, growing revenue is starting to match sky-high share prices. On February 21st Nvidia, which designs chips used to train and run models like GPT -4, reported bumper fourth-quarter results, sending its market value towards $2trn. AI mania has also lifted the share prices of other tech giants, including Alphabet (Google\u2019s corporate parent), Amazon and Microsoft, which are spending big on developing the technology.\n\nimage: The Economist\n\nAt the same time, big tech\u2019s sales of AI software remain small. In the past year AI has accounted for only about a fifth of the growth in revenues at Azure, Microsoft\u2019s cloud-computing division, and related services. Alphabet and Amazon do not reveal their AI -related sales, but analysts suspect they are lower than those of Microsoft. For the AI stockmarket boom to endure, these firms will at some point need to make serious money from selling their services to clients. Businesses across the world, from banks and consultancies to film studios, have to start using Chat GPT -like tools on a large scale. When it comes to real-world adoption of such \u201cgenerative\u201d AI , companies have trodden gingerly. Yet even these baby steps hint at the changing nature of white-collar work.\n\nimage: The Economist\n\nPrevious technological breakthroughs have revolutionised what people do in offices. The spread of the typewriter put some workers out of a job: \u201cWith the aid of this little machine an operator can accomplish more correspondence in a day than half a dozen clerks can with the pen, and do better work,\u201d said an observer in 1888. The rise of the computer about a century later eliminated some low-level administrative tasks even as it made highly skilled employees more productive. According to one paper, the computer explains over half the shift in demand for labour towards college-educated workers from the 1970s to the 1990s. More recently the rise of working from home, prompted by the covid-19 pandemic and enabled by video-conferencing, has changed the daily rhythms of white-collar types.\n\nCould generative AI prompt similarly profound changes? A lesson of previous technological breakthroughs is that, economywide, they take ages to pay off. The average worker at the average firm needs time to get used to new ways of working. The productivity gains from the personal computer did not come until at least a decade after it became widely available. So far there is no evidence of an AI -induced productivity surge in the economy at large. According to a recent survey from the Boston Consulting Group ( BCG ), a majority of executives said it will take at least two years to \u201cmove beyond the hype\u201d around AI . Recent research by Oliver Wyman, another consultancy, concludes that adoption of AI \u201chas not necessarily translated into higher levels of productivity\u2014yet\u201d.\n\nimage: The Economist\n\nThat is unsurprising. Most firms do not currently use Chat GPT , Google\u2019s Gemini, Microsoft\u2019s Copilot or other such tools in a systematic way, even if individual employees play around with them. A fortnightly survey by America\u2019s Census Bureau asks tens of thousands of businesses whether they use some form of AI . This includes the newfangled generative sort and the older type that companies were using before 2023 for everything from improving online search results to forecasting inventory needs. In February only about 5% of American firms of all sizes said they used AI . A further 7% of firms plan to adopt it within six months (see chart). And the numbers conceal large differences between sectors: 17% of firms in the information industry, which includes technology and media, say they use it to make products, compared with 3% of manufacturers and 5% of health-care companies.\n\nWhen the Census Bureau began asking about AI in September 2023, small firms were likelier to use the technology than big ones, perhaps because less form-ticking made adoption easier for minnows. Today AI is most prevalent in big companies (with more than 250 employees), which can afford to enlist dedicated AI teams and to pay for necessary investments. A poll of large firms by Morgan Stanley, a bank, found that between the start and end of 2023 the share with pilot AI projects rose from 9% to 23%.\n\nimage: The Economist\n\nSome corporate giants are frantically experimenting to see what works and what doesn\u2019t. They are hiring AI experts by the thousand, suggest data from Indeed, a job-search platform (see chart). Last year Jamie Dimon, boss of JPMorgan Chase, said that the bank already had \u201cmore than 300 AI use cases in production today\u201d. Capgemini, a consultancy, says it will \u201cutilise Google Cloud\u2019s generative AI to develop a rich library of more than 500 industry use cases\u201d. Bayer, a big German chemicals company, claims to have more than 700 use cases for generative AI .\n\nThis \u201cuse-case sprawl\u201d, as one consultant calls it, can be divided into three big categories: window-dressing, tools for workers with low to middling skills, and those for a firm\u2019s most valuable employees. Of these, window-dressing is by far the most common. Many firms are rebranding run-of-the-mill digitisation efforts as \u201cgen AI programmes\u201d to sound more sophisticated, says Kristina McElheran of the University of Toronto. Presto, a purveyor of restaurant tech, introduced a gen- AI assistant to take orders at drive-throughs. But fully 70% of such orders require a human to help. Spotify, a music-streaming firm, has rolled out an AI disc-jockey which selects songs and provides inane banter. Recently Instacart, a grocery-delivery company, removed a tool that generated photos of vendors\u2019 food, after the AI showed customers unappetising pictures. Big tech firms, too, are incorporating their own AI breakthroughs into their consumer-facing offerings. Amazon is launching Rufus, an AI -powered shopping assistant that no shopper really asked for. Google has added AI to Maps, making the product more \u201cimmersive\u201d, whatever that means.\n\nTools for lower-skilled workers could be more immediately useful. Some simple applications for things like customer service involve off-the-shelf AI . Most customers\u2019 questions are simple and concern a small number of topics, making it easy for companies to train chatbots to deal with them. A few of these initiatives may already be paying off. Amdocs produces software to help telecoms companies manage their billing and customer services. The use of generative AI , the company says, has reduced the handling time of customers\u2019 calls by almost 50%. Sprinklr, which offers similar products, says that recently one of its luxury-goods clients \u201chas seen a 25% improvement\u201d in customer-service scores.\n\nRoutine administrative tasks likewise look ripe for AI disruption. The \u201ctop examples\u201d of Bayer\u2019s 700 use cases include mundane jobs such as \u201ceasily getting data from Excel files\u201d and \u201ccreating a first draft in Word\u201d. Some companies are using generative AI as cleverer search. At Nasdaq, a financial-services firm, it helps financial-crime sleuths gather evidence to assess suspicious bank transactions. According to the company, this cuts a process which can take 30-60 minutes to three minutes.\n\nGiving AI tools to a firm\u2019s most valuable workers, whose needs are complex, is less widespread so far. But it, too, is increasingly visible. Lawyers have been among the earliest adopters. Allen & Overy, a big law firm, teamed up with Harvey, an AI startup, to develop a system that its lawyers use to help with everything from due diligence to contract analysis. Investment banks are using AI to automate part of their research process. At Bank of New York Mellon an AI system processes data for the bank\u2019s analysts overnight and gives them a rough draft to work with in the morning. \u201cSo rather than getting up at four in the morning to write research, they get up at six,\u201d the bank says. Small mercies. Sanofi, a French drugmaker, uses an AI app to provide executives with real-time information about many aspects of the company\u2019s operations.\n\nSome companies are using the technology to build software. Microsoft\u2019s GitHub Copilot, an AI coding-writing tool, has 1.3m subscribers. Amazon and Google have rival products. Apple is reportedly working on one. Fortive, a technology conglomerate, says that its operating companies \u201care seeing a greater-than-20% acceleration in software-development time through the use of gen AI \u201d. Chirantan Desai, chief operating officer of ServiceNow, a business-software company, has said that GitHub Copilot produces \u201csingle-digit productivity gains\u201d for his firm\u2019s developers. With the help of AI tools, Konnectify, an Indian startup, went from releasing four apps per month to seven. Surveys from Microsoft suggest that few people who start using Copilot want to give it up.\n\nPinterest, a social-media company, says it has improved the relevance of users\u2019 search results by ten percentage points thanks to generative AI . On a recent earnings call its boss, Bill Ready, said that new models were 100 times bigger than the ones his firm used before. L\u2019Or\u00e9al, one of the world\u2019s largest cosmetics firms, has caught the eye of investors as it improves Bet IQ , an internal tool to measure and improve the company\u2019s advertising and promotion. L\u2019Or\u00e9al claims that generative AI is already generating \u201cproductivity increases of up to 10-15% for some of our brands that have deployed it\u201d.\n\nThis does not mean that those brands will need 10-15% fewer workers. As with earlier technological revolutions, fears of an AI jobs apocalypse look misplaced. So far the technology appears to be creating more jobs than it eliminates. A survey published in November by Evercore ISI , a bank, found that just 12% of corporations believed that generative AI had replaced human labour or would replace it within 12 months. Although some tech firms claim to be freezing hiring or cutting staff because of AI , there is little evidence of rising lay-offs across the rich world.\n\nGenerative AI is also generating new types of white-collar work. Companies including Nestl\u00e9, a coffee-to-cat-food conglomerate, and KPMG , a consultancy, are hiring \u201cprompt engineers\u201d expert at eliciting useful responses from AI chatbots. One insurance firm employs \u201cexplainability engineers\u201d to help understand the outputs of AI systems. A consumer-goods firm that recently introduced generative AI in its sales team now has a \u201csales-bot manager\u201d to keep an eye on the machines.\n\nThough such developments will not translate into overall productivity statistics for a while, they are already affecting what white-collar workers do. Some effects are clearly good. AI lets firms digitise and systematise internal data, from performance reviews to meeting records, that had previously remained scattered. Respondents to surveys conducted by Randy Bean, a consultant, reported big improvements in establishing an internal \u201cdata and analytics culture\u201d, which plenty of businesses find stubbornly difficult to nurture.\n\nAI adoption may also have certain unpredictable consequences. Although AI code-writing tools are helping software engineers do their jobs, a report for GitClear, a software firm, found that in the past year or so the quality of such work has declined. Programmers may be using AI to produce a first draft only to discover that it is full of bugs or lacking concision. As a result, they could be spending less time writing code, but more time reviewing and editing it. If other companies experience something similar, the quantity of output in the modern workplace may go up\u2014as AI churns out more emails and memos\u2014even as that output becomes less useful for getting stuff done.\n\nPolling by IBM , a tech firm, suggests that many companies are cagey about adopting AI because they lack internal expertise on the subject. Others worry that their data is too siloed and complex to be brought together. About a quarter of American bosses ban the use of generative AI at work entirely. One possible reason for their hesitance is worry about their companies\u2019 data. In their annual reports Blackstone, a private-equity giant, and Eli Lilly, a pharmaceutical one, have warned investors about AI -related risks such as possible leakage of intellectual property to AI model-makers. Last year Marie-H\u00e9l\u00e8ne Briens Ware, an executive at Orange, a telecoms company, explained that the firm had put data guardrails in place before commencing a trial with Microsoft\u2019s Copilot.\n\nUltimately, for more businesses to see it as an open-and-shut case, generative AI still needs to improve. In November Microsoft launched a Copilot for its productivity software, such as Word and Excel. Some early users find it surprisingly clunky and prone to crashing\u2014not to mention cumbersome, even for people already adept at Office. Many bosses remain leery of using generative AI for more sensitive operations until the models stop making things up. Recently Air Canada found itself in hot water after its AI chatbot gave a passenger incorrect information about the airline\u2019s refund policy. That was embarrassing for the carrier, but it is easy to imagine something much worse. Still, even the typewriter had to start somewhere. \u25a0\n\nTo stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.", "publisher": {"href": "https://www.economist.com", "title": "The Economist"}}, {"title": "Will AI Be Taking Over Your Photography?", "top_image": "https://cdn.fstoppers.com/styles/large-16-9/s3/lead/2024/02/dsc09927-copy.jpeg", "images": ["data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7", "https://www.facebook.com/tr?id=305312604074986&ev=PageView&noscript=1", "https://cdn.fstoppers.com/styles/full/s3/media/2024/02/23/screenshot_2024-02-22_233023.png", "https://cdn.fstoppers.com/styles/full/s3/media/2024/02/23/img_6594.jpeg", "data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20150%2087%22%3E%3C/svg%3E", "https://cdn.fstoppers.com/styles/square_medium/s3/avatars/2020/09/nv_profile2020-01.jpg?c=43d0a121ce32af5d6bf9ff14d9a48f46", "https://cdn.fstoppers.com/styles/full/s3/media/2024/02/23/screenshot_2024-02-22_232144.png", "https://cdn.fstoppers.com/styles/full/s3/media/2024/02/23/img_6593.jpeg", "https://cdn.fstoppers.com/styles/large-16-9/s3/lead/2024/02/dsc09927-copy.jpeg", "https://cdn.fstoppers.com/styles/full/s3/media/2024/02/23/screenshot_2024-02-22_232553.png", "https://cdn.fstoppers.com/styles/full/s3/media/2024/02/23/screenshot_2024-02-22_232259.png"], "videos": ["https://www.youtube.com/embed/QiH8XpgpxZw?feature=oembed"], "url": "https://fstoppers.com/artificial-intelligence/will-ai-be-taking-over-your-photography-659207", "date": "Sat, 24 Feb 2024 17:04:01 GMT", "short_description": "Will AI Be Taking Over Your Photography?  Fstoppers", "text": "Did artificial intelligence technology take over photography? No, it hasn\u2019t, and it probably never will. But here is how AI might have been helping you behind the scenes all along.\n\nArtificial intelligence, or AI, is perhaps the biggest buzzword in the world of photography and tech for 2023, and it continues to be a widely discussed topic in 2024. This stems from the fact that the industry has had mixed reactions about the emergence of AI tools for photography and raised the question of whether this kind of technology puts photographers' jobs at risk. However, much of the alarm and resistance towards this kind of technology often comes from photographers who have not fully understood what AI actually is and what it actually does. The discussion on that can go way too deep into technicalities, but for the benefit of a simple discussion, let\u2019s start by saying that AI will not mean robots will be taking over our jobs.\n\nThe video below is a discussion among myself, landscape and travel photographer Elia Locardi, and portrait and wedding photographer Jiggie Alejandrino, where we talk about how AI has affected our post-production workflow.\n\nArtificial Intelligence in Different Forms\n\nAI comes in various forms that have current applications in a photographer\u2019s workflow. But from a wider perspective of technology, artificial intelligence can be seen as just a more advanced and more intricate level of computing processes that can be applied to anything and everything we do on digital devices. In a sense, the technology is not new at this point; at most, it has just matured to become more capable. AI was introduced in smartphone cameras at least seven years ago as a form of enhanced image processing through scene detection and local adjustments. This early form of AI in smartphone cameras was an improved version of the automatic mode that almost every camera has. It offered better metering and color processing through scene detection, which in effect just made it easier for smartphone photographers to take better photos without having to edit or refine so much as it uses cues to detect the scene and match applicable settings and adjustments for the detected scene. In recent months, AI-backed editing features have been made on smartphones, such as Samsung\u2019s new S24 series, where one could easily remove objects and even subjects from the photos taken by the phone.\n\nAI on the Asus Zenfone 5 released in 2018 supposedly used artificial intelligence to detect scenes and automatically adjust exposure and color settings\n\nIn conventional interchangeable lens cameras, Sony introduced AI processing in its focusing system, which first appeared on the Sony A7RV. From personal use, it was apparent that this form of AI boosts the capabilities of the processor to detect and track the selected subject through eye-AF tracking. Instead of simply detecting where the eye of the subject is and subsequently focusing on it, the AI processor is now able to predict and anticipate the motion of the moving subject to be able to track and focus faster, thereby reducing the probability of missing focus on some frames. This has been made available on other cameras that were released after it and is also now being offered by other camera brands.\n\nAn illustration from Sony.co.uk showing the AI recognition and detection for focus tracking\n\nAI in Post-Production\n\nOn the other hand, AI takes more diverse forms in post-production. Various tools have emerged that introduce AI processes into different steps of post-production. Some tools use AI to automate the process of developing raw images. These tools use AI-powered scene detection to apply exposure, color, and contrast treatment that is appropriate for the specific shot. Some tools use AI to detect commonly removed skin elements in the retouching process to reduce the amount of time spent editing portraits. Some tools offer exponentially greater precision in cloning out and filling in identified spots on a surface.\n\nAI Post-processing doing global adjustments through Radiant Photo\n\nWhat needs to be said about AI in editing is that almost none of these tools are actually new. For decades now, photographers have been using tools like content-aware fill, which is just a more intuitive version of older tools such as the heal tool or the patch tool. With smarter AI capabilities, this has now evolved into the remove tool, which is just a much smarter version of the earlier tools that is more effective in preserving the details and continuity of visual elements that it is being used on.\n\nThe remove tool acting as a smarter content aware fill for retouching\n\nAs a landscape and architectural photographer, I personally experienced having to trace out the sky in images using tools like the pen tool, the magic wand, or the lasso tool for hours on a single image just to be able to selectively edit them or blend in a new sky. In the past couple of years, this was made easier by a smarter \u201cselect sky\u201d option or a semi-automated sky replacement module that allowed the user to select any image and use it as the sky on the photo being worked on, and that includes images (supposedly) legally acquired to be used for that purpose. This ultimately automated the process that used to take hours and compressed it into a few seconds. Now with the use of those tools and with generative fill, the process is made much more efficient and infuses options generated from what are supposedly consensually referenced images. All of that said, all of these were technically possible with Photoshop in the past; the difference lies in the speed and precision with which the tasks are performed.\n\nFeatures that were available before \u201cAI\u201d was properly named now seem more detail oriented and intuitive.\n\nGenerative AI and Photography\n\nPerhaps the form of AI that causes much of the worry would be the generative form of AI that has the ability to create illustrations either as standalone images or as part of a photograph that is being edited. It is undeniable that there have been instances where AI-generated images were maliciously used in photography competitions. There have also been instances where commercial campaigns were called out for using AI-generated illustrations, specifically for brands that sell tools for digital artists. It is undeniable that, in the wrong hands, AI can be used to cheat, mislead, or undermine photographers and, on a larger scale, it can be used for spreading disinformation.\n\nHowever, at this point, it does not seem like AI tools can do anything that human creatives cannot. If anything, they can only do them faster but not necessarily better. The core of AI is to emulate human responses to automate what was previously only doable by human work, but it seems (at least from how I see it) that the effect of what is done with AI depends entirely on the intention of whoever creates with it or whoever uses the images. That then simplifies this sophisticated artificial intelligence technology created from learned neural responses as merely a tool. In the same way that a hammer can be used both to build and to destroy, AI tools, from this perspective, remain to be what they are \u2014 tools. This is where laws, regulations, and ethics need to step in; to prevent this new tool from being used to cause the same old harm that we have been preventing in the past.\n\nImplications of AI in Photography\n\nThe possibilities that come with AI are endless and almost impossible to bind, as with any other form of advancement in technology. It can definitely lead to unfortunate circumstances when used improperly, which is where the importance of human intervention, regulation, and government steps in. When it comes to how generative AI can be used for malicious things, it is up to us collectively to step in and prevent this.\n\nGenerative fill being used to edit minor details through Photoshop\n\nOn the other hand, AI tools in photography do offer a lot of possibilities. As with any other tool, AI can offer much better efficiency in our work as photographers. In-camera, AI has been helpful in minimizing the role of luck and increasing precision in our cameras. In post-production, AI tools can offer to do days' worth of work in only a matter of minutes if properly used and prompted to achieve the correct results. Any photographer who shoots raw can be spared from hours of raw developing or using presets that still need refining, but this can only happen if the photographer has developed a consistent style and intentionality. Portrait photographers can now more efficiently retouch their subjects based on their style, preference, and the extent of processing they choose. Commercial photographers can now do everything they\u2019ve been doing on Photoshop for the past couple of decades with much more precision while only taking a minuscule fraction of the time.\n\nAuthenticity Will Never Lose Value\n\nAs a landscape photographer, I can say that in some way of looking at it, landscape photography was as seemingly threatening as AI is now to landscape painters and visual artists before the dawn of photography. However, there seems to be barely any conflict between landscape painters and landscape photographers because the demand for either of them is just based on the preference of an entirely different audience. At the same time, as an architectural photographer, the most constant challenge is getting a client to spend more for photography of the actual structure instead of just using the already-available design perspective that was there way before the building was erected. In this case, it doesn\u2019t seem like AI-generated images will be any more of a threat than other ways of illustrating an architectural project.\n\nFor portrait photographers, wedding photographers, event photographers, even more so, do I personally believe that authenticity, specifically the value of having genuine images of a person or event, probably won\u2019t decline. AI, specifically generative AI, will definitely cause and actually has caused ethical dilemmas in terms of the crisis of disinformation and misinformation, but overall, photography is the art form that has the most potential to uphold truth and authenticity even in the age of advanced technology, for as long as the line between art and documentation is clearly maintained.\n\nBut for an individual photographer, with this not-so-new advancement in technology, and with human intervention factored in, AI tools offer two options. We can either spend less time editing so that we can take in more clients and shoot more, or we can spend less time editing so that we can spend more time for ourselves outside of work. If anything, this might be a reminder that most of us probably started learning photography for the sake of taking pictures \u2014 not editing them, so technology is now offering to take the bulk of that (still) necessary step of the process so that we could focus on the creative aspect.", "publisher": {"href": "https://fstoppers.com", "title": "Fstoppers"}}, {"title": "How AI Is Already Transforming the Media - POLITICO", "top_image": "https://www.politico.com/news/magazine/2024/02/27/artificial-intelligence-media-00143508/favicon.ico", "images": [], "videos": [], "url": "https://www.politico.com/news/magazine/2024/02/27/artificial-intelligence-media-00143508", "date": "Tue, 27 Feb 2024 15:23:10 GMT", "short_description": "How AI Is Already Transforming the Media  POLITICO", "text": "How AI Is Already Transforming the Media  POLITICO", "publisher": {"href": "https://www.politico.com", "title": "POLITICO"}}, {"title": "Using AI To Modernize Drug Development And Lessons Learned - Forbes", "top_image": "https://www.forbes.com/sites/cindygordon/2024/02/23/using-ai-to-modernize-drug-development-and-lessons-learned//favicon.ico", "images": [], "videos": [], "url": "https://www.forbes.com/sites/cindygordon/2024/02/23/using-ai-to-modernize-drug-development-and-lessons-learned/", "date": "Fri, 23 Feb 2024 23:35:41 GMT", "short_description": "Using AI To Modernize Drug Development And Lessons Learned  Forbes", "text": "Using AI To Modernize Drug Development And Lessons Learned  Forbes", "publisher": {"href": "https://www.forbes.com", "title": "Forbes"}}, {"title": "Phone App Uses AI to Detect Depression From Facial Cues", "top_image": "https://home.dartmouth.edu/sites/home/files/styles/16_9_lg/public/2024-02/20240130-MoodCaputure-Team-kl-4.jpg?h=10d202d3&itok=Va6Fq5du", "images": ["https://news.google.com/sites/home/files/styles/max_width_560px/public/2021-11/About-Dropdown-Subscribe-CTA.jpg?itok=J9dPLev7", "https://news.google.com/sites/home/files/styles/max_width_560px/public/2023-12/WBB_UAlbany-15.jpg?itok=zOftGa77", "https://news.google.com/sites/home/files/styles/max_width_720px/public/2024-02/20240130-MoodCaputure-Team-kl-4.jpg?itok=XhEZKTcr", "https://home.dartmouth.edu/sites/home/files/styles/16_9_lg/public/2024-02/20240130-MoodCaputure-Team-kl-4.jpg?h=10d202d3&itok=Va6Fq5du", "https://news.google.com/sites/home/files/styles/max_width_560px/public/2024-01/1-20240108-campus-snow-aerial-cj-15.jpg?itok=xkLr1XgD"], "videos": [], "url": "https://home.dartmouth.edu/news/2024/02/phone-app-uses-ai-detect-depression-facial-cues", "date": "Tue, 27 Feb 2024 14:18:51 GMT", "short_description": "Phone App Uses AI to Detect Depression From Facial Cues  Dartmouth News", "text": "Body\n\nDartmouth researchers report they have developed the first smartphone application that uses artificial intelligence paired with facial-image processing software to reliably detect the onset of depression before the user even knows something is wrong.\n\nCalled MoodCapture, the app uses a phone\u2019s front camera to capture a person\u2019s facial expressions and surroundings during regular use, then evaluates the images for clinical cues associated with depression. In a study of 177 people diagnosed with major depressive disorder, the app correctly identified early symptoms of depression with 75% accuracy.\n\nThese results suggest the technology could be publicly available within the next five years with further development, say the researchers, who are based in the Department of Computer Science and Geisel School of Medicine.\n\nThe team published their findings on the arXiv preprint database on Feb. 27 in advance of presenting it at the Association of Computing Machinery\u2019s CHI 2024 conference in May. Papers presented at CHI are peer-reviewed prior to acceptance and will be published in the conference proceedings.\n\n\u201cThis is the first time that natural \u2018in-the-wild\u2019 images have been used to predict depression,\u201d says Andrew Campbell, the paper\u2019s corresponding author and the Albert Bradley 1915 Third Century Professor of Computer Science. \u201cThere\u2019s been a movement for digital mental-health technology to ultimately come up with a tool that can predict mood in people diagnosed with major depression in a reliable and nonintrusive way.\u201d\n\n\u201cPeople use facial recognition software to unlock their phones hundreds of times a day,\u201dsays Campbell, whose phone recently showed he had done so more than 800 times in one week.\n\n\u201cMoodCapture uses a similar technology pipeline of facial recognition technology with deep learning and AI hardware, so there is terrific potential to scale up this technology without any additional input or burden on the user,\u201d he says. \u201cA person just unlocks their phone and MoodCapture knows their depression dynamics and can suggest they seek help.\u201d\n\nFor the study, the application captured 125,000 images of participants over the course of 90 days. People in the study consented to having their photos taken via their phone\u2019s front camera but did not know when it was happening.\n\nQuote My feeling is that technology such as this could be available to the public within five years. We\u2019ve shown that this is doable. Attribution Andrew Campbell, Albert Bradley 1915 Third Century Professor of Computer Science\n\nA first group of participants was used to program MoodCapture to recognize depression. They were photographed in random bursts using the phone\u2019s front-facing camera as they responded to the statement, \u201cI have felt down, depressed, or hopeless.\u201d The prompt is from the eight-point Patient Health Questionnaire, or PHQ-8, which is used by clinicians to detect and monitor major depression.\n\nThe researchers used image-analysis AI on these photos so that MoodCapture\u2019s predictive model could learn to correlate self-reports of feeling depressed with specific facial expressions\u2014such as gaze, eye movement, positioning of the head, and muscle rigidity\u2014and environmental features such as dominant colors, lighting, photo locations, and the number of people in the image.\n\nThe concept is that every time a user unlocks their phone, MoodCapture analyzes a sequence of images in real time. The AI model draws connections between expressions and background details found to be important in predicting the severity of depression. Over time, MoodCapture identifies image features specific to the user. For example, if someone consistently appears with a flat expression in a dimly lit room for an extended period, the AI model might infer that person is experiencing the onset of depression.\n\nTo test the predictive model, the researchers had a separate group of participants answer the same PHQ-8 question while MoodCapture photographed them. The software analyzed these photos for indicators of depression based on the data collected from the first group. It is this second group that the MoodCapture AI correctly determined were depressed or not with 75% accuracy.\n\n\u201cThis demonstrates a path toward a powerful tool for evaluating a person\u2019s mood in a passive way and using the data as a basis for therapeutic intervention,\u201d says Campbell, noting that an accuracy of 90% would be the threshold of a viable sensor. \u201cMy feeling is that technology such as this could be available to the public within five years. We\u2019ve shown that this is doable.\u201d\n\nMoodCapture meets major depression on the irregular timescale on which it occurs, said Nicholas Jacobson, a study co-author and assistant professor of biomedical data science and psychiatry in Dartmouth\u2019s Center for Technology and Behavioral Health.\n\n\u201cMany of our therapeutic interventions for depression are centered around longer stretches of time, but these folks experience ebbs and flows in their condition. Traditional assessments miss most of what depression is,\u201d said Jacobson, who directs the AI and Mental Health: Innovation in Technology Guided Healthcare (AIM HIGH) Laboratory.\n\n\u201cOur goal is to capture the changes in symptoms that people with depression experience in their daily lives,\u201d Jacobson says. \u201cIf we can use this to predict and understand the rapid changes in depression symptoms, we can ultimately head them off and treat them. The more in the moment we can be, the less profound the impact of depression will be.\u201d\n\nJacobson anticipates that technologies such as MoodCapture could help close the significant gap between when people with depression need intervention and the access they actually have to mental health resources. On average, less than 1% of a person\u2019s life is spent with a clinician such as a psychiatrist, he says. \u201cThe goal of these technologies is to provide more real-time support without adding an additional pressure on the care system,\u201d Jacobson says.\n\nAn AI application like MoodCapture would ideally suggest preventive measures such as going outside or checking in with a friend instead of explicitly informing a person they may be entering a state of depression, Jacobson says.\n\n\u201cTelling someone something bad is going on with them has the potential to make things worse,\u201d he says. \u201cWe think that MoodCapture opens the door to assessment tools that would help detect depression in the moments before it gets worse. These applications should be paired with interventions that actively try to disrupt depression before it expands and evolves. A little over a decade ago, this type of work would have been unimaginable.\u201d\n\nThe study stems from a National Institutes of Mental Health grant Jacobson leads that is investigating the use of deep learning and passive data collection to detect depression symptoms in real time. It also builds off a 2012 study led by Campbell\u2019s lab that collected passive and automatic data from the phones of participants at Dartmouth to assess their mental health.\n\nBut the advancement of smartphone cameras since then allowed the researchers to clearly capture the kind of passive photos that would be taken during normal phone usage, Campbell says. Campbell is director of emerging technologies and data analytics in the Center for Technology and Behavioral Health, where he leads the team developing mobile sensors that can track metrics such as emotional state and job performance based on passive data.\n\nThe new study shows that passive photos are key to successful mobile-based therapeutic tools, Campbell says. They capture mood more accurately and frequently than user-generated selfies and do not deter users by requiring active engagement. \u201cThese neutral photos are very much like seeing someone in-the-moment when they\u2019re not putting on a veneer, which enhanced the performance of our facial-expression predictive model,\u201d Campbell says.\n\nSubigya Nepal, a Guarini School of Graduate and Advanced Studies PhD candidate in Campbell\u2019s research group who, along with Guarini PhD student Arvind Pillai is co-lead author of the study, says the next steps for MoodCapture include training the AI on a greater diversity of participants, improving its diagnostic ability, and reinforcing privacy measures.\n\nThe researchers envision an iteration of MoodCapture for which photos never leave a person\u2019s phone, Nepal says. Pictures would instead be processed on a user\u2019s device to extract facial expressions associated with depression and convert them into code for the AI model. \u201cEven if the data ever does leave the device, there would be no way to put it back together into an image that identifies the user,\u201d he says.\n\nMeanwhile, the application\u2019s accuracy could be enhanced on the consumer end if the AI is designed to expand its knowledge based on the facial expressions of the specific person using it, Nepal says.\n\n\u201cYou wouldn\u2019t need to start from scratch\u2014we know the general model is 75% accurate, so a specific person\u2019s data could be used to fine-tune the model. Devices within the next few years should easily be able to handle this,\u201d Nepal says. \u201cWe know that facial expressions are indicative of emotional state. Our study is a proof of concept that when it comes to using technology to evaluate mental health, they\u2019re one of the most important signals we can get.\u201d", "publisher": {"href": "https://home.dartmouth.edu", "title": "Dartmouth News"}}, {"title": "ChatGPT in medical education: Generative AI and the future of artificial intelligence in health care", "top_image": "https://www.ama-assn.org/sites/ama-assn.org/files/2024-02/au-social-022324.jpg", "images": ["https://news.google.com/themes/custom/ama_one/images/brand/logo-reversed.svg", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2023-02/2022-05-02-SOAPREFLECT.jpg?itok=cvVPNJ9B", "https://news.google.com/themes/custom/ama_one/logo.svg", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2024-02/AMBOSS-Web-Hero_1170x780.png?itok=Hgi9dkFi", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2023-03/2023-03-14-NUTRIWISHLIST-Index-1170x780.jpg?itok=xo5LPsA0", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2024-02/navigating-ai-health-care.png?itok=fw12pX5a", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2022-12/nurse-practitioner-physician.JPG?itok=hbK4by2w", "https://www.ama-assn.org/sites/ama-assn.org/files/2024-02/au-social-022324.jpg", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2022-08/2022-07-13-NETWORKING-Index_1170x780.jpg?itok=fD_4Yo6A", "https://news.google.com/sites/ama-assn.org/files/styles/related_article_stub_image_1200x800_3_2/public/2024-02/2024-01-16-ACADEHR_Index-1170x780.jpg?itok=qD7r0JUD", "https://news.google.com/sites/ama-assn.org/files/styles/related_article_stub_image_1200x800_3_2/public/2024-02/2024-02-02-DIFFND_Index-1170x780.jpg?itok=J_W2-syl", "https://news.google.com/sites/ama-assn.org/files/styles/related_article_stub_image_1200x800_3_2/public/2020-03/Cartoon-of-undecided-man.jpg?itok=VKBWab9u", "https://news.google.com/sites/ama-assn.org/files/styles/related_article_stub_image_1200x800_3_2/public/2024-02/2024-01-30-PROSTATE_Index-1170x780.jpg?itok=7uTeU2-m", "https://news.google.com/sites/ama-assn.org/files/styles/membership_promo_image/public/2022-10/22-738178-Updated-podcast-art-AMA-Update-2_0.png?itok=C1C4MtME", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2019-03/2019-02-15-index-MATCHB.jpg?itok=7ZVpWcC9", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2024-02/2024-02-02-DOCTORSACT_Index-1170x780.jpg?itok=8neRKJ6u", "https://news.google.com/sites/ama-assn.org/files/styles/article_stub_800_x_600/public/2024-02/2024-02-01-SAS24NONCOMPETE_Index-1170x780.jpg?itok=KfqybAH_"], "videos": [], "url": "https://www.ama-assn.org/practice-management/digital/chatgpt-medical-education-generative-ai-and-future-artificial", "date": "Fri, 23 Feb 2024 15:33:45 GMT", "short_description": "ChatGPT in medical education: Generative AI and the future of artificial intelligence in health care  American Medical Association", "text": "AMA Update covers a range of health care topics affecting the lives of physicians, residents, medical students and patients. From private practice and health system leaders to scientists and public health officials, hear from the experts in medicine on COVID-19, medical education, advocacy issues, burnout, vaccines and more.\n\nFeatured topic and speakers Featured topic and speakers\n\nWhy should AI be in classrooms? How do you integrate AI into a curriculum? AI tools for medical education, using AI in medical training and how AI can help you study.\n\nOur guest is Marc Triola, MD, associate dean of education informatics and director of the Institute for Innovations in Medical Education at NYU Grossman School of Medicine. AMA Chief Experience Officer Todd Unger hosts.\n\nSpeaker\n\nMarc Triola, MD, associate dean, education informatics; director, Institute for Innovations in Medical Education, NYU Grossman School of Medicine\n\nAMA Recovery Plan for America\u2019s Physicians After fighting for physicians during the pandemic, the AMA is taking on the next extraordinary challenge: Renewing the nation\u2019s commitment to physicians. Let's Get Started\n\nTranscript Transcript\n\nUnger: Hello and welcome to the AMA Update video and podcast. Today, we're talking about the ways in which AI is changing medical education. I'm joined today by Dr. Marc Triola, associate dean of education informatics and director of the Institute for Innovations in Medical Education at NYU Grossman School of Medicine in New York. I'm Todd Unger, AMA's chief experience officer in Chicago. Dr. Triola, welcome.\n\nDr. Triola: Thank you. It's great to be here and thanks for the opportunity.\n\nUnger: Well, it seems like a lot longer than a year, but it has been about a year since ChatGPT got everybody talking about AI. At NYU Grossman, you're already using this technology to support students in brand new ways and we're eager to hear more about that. But, first, can you tell us what inspired you to leverage AI at your institution in the first place?\n\nDr. Triola: Well, I think like everyone around the world, we were struck by the power of AI, the rapidity by which these innovations were being developed, how accurate and realistic the text they were generating was, how well it handled understanding concepts in text, and we saw an application for AI to improve the work of our students, our faculty, our clinicians. And as an as a health system, we're really embracing AI across all three of our missions\u2014our mission to teach medical students and residents, to care for our patients, and to discover new science. And we think that there are a huge number of applications to treat many of the pain points in, not just medical education, but the delivery of care.\n\nUnger: It's interesting because so much of the discussion over the past year has been about concern about how students are going to use AI, so we're really talking about a different thing. So let's take a closer look at the ways that you're using AI, starting with how it's helping your students learn and access research. Tell us more about that.\n\nDr. Triola: Sure. One of the first projects that we did was a project called DX Mentor where we are using AI to automatically identify learning resources and the latest medical literature based on the diagnoses of the patients they're caring for. We built a system here that connects data from our Epic electronic health record, and it understands which patients in Epic are being cared for by our medical students, connecting that data with a digital catalog of educational resources\u2014some of these things are infographics, are videos about clinical diagnoses and the medical literature itself\u2014all of the latest guideline and review papers that are published in PubMed.\n\nThis system every day curates a custom set of nudges for each of our students that delivers to them recommended learning and recommended readings for the patient they had admitted the day before or the night before. It's just in time, it's curated completely by the AI\u2014no human intervention needed\u2014and it's customized for every student. So it's a nice example of precision medical education enabled by the power of generative AI and artificial intelligence systems.\n\nDr. Triola: Wow, really interesting and so practical to their experience with patient care. Another way that you're using AI is to help students study. I'm sure that's welcome news to your students. How does that exactly work?\n\nUnger: Sure. And\u2014so we're experimenting in this area and I want to point out that medical students across the country are experimenting with this. And medical students have been doing a fantastic job of organizing themselves, sharing prompts for the AI to generate USMLE-type questions, flashcards to study. They're even using the AI to create virtual patients that they can interview and diagnose and get feedback from the AI on their performance in gathering a history or being empathic with patients.\n\nWe've created a similar system here that curates publicly available data sets of self-study questions, the educational resources that I mentioned before, to connect the dots between what our students need at the individual level and this growing amount of digital educational resources. And the glue that can tie them all together so efficiently these days is AI and that's\u2014so we're working very hard to implement that to really provide our students with a very precise and personalized suite of assessment and curricular items to help them learn and study in as frictionless a way as possible.\n\nMembership Moves Medicine\u2122 Free access to JAMA Network\u2122 and CME\n\nSave hundreds on insurance\n\nFight for physicians and patient rights Join the AMA Today\n\nUnger: Gosh, I love that idea of the virtual patient and diagnosis. What a great example of that. In addition to all the studying, of course, med school is also a times for students to think about their career goals. And I know, having talked to a lot of students, what's on their mind the moment they get into medical school\u2014great news is that they can now use AI to help them with this task. What can you tell us about how it's being used there?\n\nDr. Triola: So the use of AI to support our students with their goal setting and achieving their goals was an area that was surprising and powerful. Our students author goals that they share with their coaches in a custom software application that we've built here to support our students to help our faculty coaches over their time during medical school. And we've added ChatGPT to that as well.\n\nWhen our students are writing goals, they have the ability to click a button and get feedback from ChatGPT suggestions on how they might accomplish that goal. And what we found is that by no means do the suggestions from ChatGPT replace the advice that they get from that faculty member who knows them well, but they are often very concrete and detailed suggestions\u2014which journals should they read if they're interested in a particular a specialty.\n\nChatGPT even knows all about the clinics at our hospital and can suggest individual faculty at NYU or clinics that a student might want to shadow in or do research with if they want to learn more about a particular topic. So in this case it's been a nice supplement to our human coaches and a way that AI, at least in the education sphere, can begin to really enhance the ability of our faculty and our students to better leverage their time and make decisions quickly, and efficiently, and accurately about their career planning.\n\nUnger: Really interesting. I think the word that is kept popping up as we've talked about AI with several folks over the past week has been about augmenting. Is that how you see it playing a role at your institution?\n\nDr. Triola: Absolutely, yeah. AI is about giving everybody superpowers. It's not about replacing doctors, it's not about replacing medical students, it's not even about replacing the skills that they need to learn. It's about empowering everyone to be at the top of their game and to improve the quality, efficiency, consistency of the work that they are going to be doing as people, as humans. And so really that that's our approach. This is about giving everybody superpowers.\n\nUnger: I love that. And it's so refreshing to hear practical applications, the way that you're helping students in their experience because so much of the discussion around ChatGPT or AI is, at least on one end, about uncertainty and on the other end fear and so, wow, what a lot to have accomplished in such a short period of time. I'm curious what\u2014as you look ahead, what's the next big opportunity to incorporate AI into medical education?\n\nDr. Triola: Well, I think AI is going to really help us accomplish this mission of precision medical education. We want to deliver the right education to the right learner at the right time. And this is a mission that we are happy to partner with the AMA in. This has always been difficult because having that customization for every student takes a lot of work and the AI systems can really take much of that work off of our plate.\n\nAI in Health care on AMA Ed Hub\u2122 Artificial and augmented intelligence are rapidly changing technologies with wide-reaching medical implications. Earn CME credits while learning at your own pace. Start the Course Now\n\nSo I think in the future we're going to see medical education models that have AI as a co-pilot sitting next to the student, sitting next to their faculty and coaches, providing guidance and advice along the way, curating curricula and assessments, and really tailoring to\u2014at the level of that individual student tailoring what they learn and how they learn so that they make the absolute best use of their time. And it's not about skipping tasks or a shortcut to clinical competence, it's about efficiently tailoring medical education. And it's about meeting the students where they want to be in terms of their goals. And this is going to be a really key way to do it.\n\nI'd say the second point that is critical, that's going to change is that we're going to teach medical students how to\u2014need to teach medical students how to use AI clinically. This will become a key part of the practice of medicine, whether it is AI to diagnose patients, AI to communicate with patients, or AI to actually write some of the notes and do clinical documentation or other things that we haven't even imagined yet. So creating students who are prepared for that future, who understand the powers the limitations, when they should really question or really think very carefully about the recommendations coming from AI is going to be really key. And this is another area that schools are really coming together, thinking about what that curriculum should be, and how we should integrate AI in the curriculum from that clinical perspective.\n\nUnger: Now, it's interesting, you're in year one of this, you're already making these kinds of advances, you're talking about how they're the education of today's medical students is going to differ. Project yourself out 10 years and now you've had successive generations of incorporation of this technology into medical education, do you think it's going to look really different 10 years now\u2014from now than it does currently?\n\nDr. Triola: I think it will look evolved\u2014significantly evolved from where we are now. There have been many trends in medical education over the past decade that have been slowed by the pace by which faculty, by which the schools themselves have been able to implement these things. They've been slowed by our ability to perform medical education research to understand what approaches or techniques work best at producing competence and excellence in our students. And AI is going to speed all of these things up.\n\nSo I think that when we think about accelerating change in medical education, which is an initiative from the AMA, and we think about the initiatives from all of the different medical schools that are going on towards competency-based medical education, towards different models of medical education that aren't necessarily of the same fixed length or duration, that are really focused on graduating students who are at that level of competence that they were aiming for. I think that AI is really going to get us there. Although, as you point out, it's only been a year. So maybe what we'll see 10 years from now is something that is quite dramatically different.\n\nI think one of the biggest important points that we should recognize is that 10 years from now the students entering medical school will have had the benefit throughout all of elementary school, high school and college of AI. And so that may dramatically change how and what our students are learning before they're even getting to us, which could simply change the whole baseline of where medical education is. I'm hugely optimistic and excited about this. I think this is really going to unlock many of the changes and many of the opportunities that we've been trying to strive for for years.\n\nUnger: That is a really exciting vision and so interesting to hear your perspective and the progress that you've made already at your institution. Thank you so much for joining us, Dr. Triola. And we'll look forward to hearing more about your next set of innovations.\n\nIf you're interested in accelerating medical education and ensuring that technology is an asset and not a burden to future physicians, then join the AMA and make sure that happens. You can do that by going to ama-assn.org/join. That wraps up today's episode and we'll be back soon with another AMA Update. Be sure to subscribe for new episodes and find all our videos and podcasts at ama-assn.org/podcasts. Thanks for joining us today. Please take care.\n\nDisclaimer: The viewpoints expressed in this video are those of the participants and/or do not necessarily reflect the views and policies of the AMA.\n\nSubscribe to AMA Update Get videos with expert opinions from the AMA on the most important health care topics affecting physicians, residents, medical students and patients\u2014delivered to your inbox. Subscribe Now", "publisher": {"href": "https://www.ama-assn.org", "title": "American Medical Association"}}]}